# What does it actually do?
 - isolation
   - limited view of host
   - can enforce resource limits
 - ease of use
   - greater than the sum of its parts
 - portability
   - container runs wherever docker engine runs *
 - lower overhead **
   - single kernel
   - no emulation







## caveat
 - * containers are os and architecture specific; containers built for docker/arm64 won't work on docker/amd64. Some config options may limit portability.  These are exceptions though.
 - ** Lower overhead compared to virtual machines.  Network performance is dependent on configuration but can reach native speed.


## my notes

 - isolation
   - similar to a VM, containers may have their own process tree, file system, memory, network, users, etc.
   - host resources may be shared into the container (file systems, network and block devices, etc)
   - resource use can be limited by host
 - ease of use
   - docker is built on linux kernel primitives and other tools
   - you could achieve similar functionality using the components, but docker is much easier
 - portability
   - a dockerized application will run on any host capable of running docker (os, arch, edge cases)
 - lower overhead
   - compared to a VM, docker is faster and smaller in both memory and disk
   - transparent view of containers means easier memory management for kernel

 - Does anyone care about the specifics on how it works or should we move on?

